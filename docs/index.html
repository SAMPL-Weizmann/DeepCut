<!doctype html>
<html lang="en">
  <head>
    
    <link rel="stylesheet" href="style.css">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DeepCut</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
    <style>
      a:link {
         color: rgb(0, 0, 0);
         background-color: transparent;
         text-decoration: none;
      }
  </style>
  </head>
  <body>
    <p class="title">DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering</p>
    <p class="author">
      <span class="author"><a target="_blank" href="https://www.linkedin.com/in/amit-aflalo-8792b7215/">Amit Aflalo</a></span>
      <span class="author"><a target="_blank" href="https://www.weizmann.ac.il/math/bagon/home">Shai Bagon</a></span>
      <span class="author"><a target="_blank" href="https://www.linkedin.com/in/tamar-kashti-35a8071">Tamar Kashti</a></span>
      <span class="author"><a target="_blank" href="https://www.weizmann.ac.il/math/yonina/about-me/yonina-eldar">Yonina Eldar</a></span>

    </p>
    <div id ="center_img">
      <a href="https://www.weizmann.ac.il/math/yonina/signal-acquisition-modeling-processing-and-learning-sampl-lab" target="_blank"><img src="images/SAMPLLAB.png" style="width:17.5%"></a>
      <a href="https://www.weizmann.ac.il/" target="_blank"><img src="images/wis2.jpg" style="width:30%"></a>
    </div>

    <p class="icons">
      <button onclick="location.href='https://github.com/SAMPL-Weizmann/DeepCut'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
      <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
      </svg>
        GitHub
      </button>
      <button onclick="location.href='https://arxiv.org/abs/2212.05853'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-newspaper" viewBox="0 0 16 16">
      <path d="M0 2.5A1.5 1.5 0 0 1 1.5 1h11A1.5 1.5 0 0 1 14 2.5v10.528c0 .3-.05.654-.238.972h.738a.5.5 0 0 0 .5-.5v-9a.5.5 0 0 1 1 0v9a1.5 1.5 0 0 1-1.5 1.5H1.497A1.497 1.497 0 0 1 0 13.5v-11zM12 14c.37 0 .654-.211.853-.441.092-.106.147-.279.147-.531V2.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5v11c0 .278.223.5.497.5H12z"></path>
      <path d="M2 3h10v2H2V3zm0 3h4v3H2V6zm0 4h4v1H2v-1zm0 2h4v1H2v-1zm5-6h2v1H7V6zm3 0h2v1h-2V6zM7 8h2v1H7V8zm3 0h2v1h-2V8zm-3 2h2v1H7v-1zm3 0h2v1h-2v-1zm-3 2h2v1H7v-1zm3 0h2v1h-2v-1z"></path>
      </svg>
        Paper
      </button>
      <button onclick="location.href='https://colab.research.google.com/drive/1LTz2TuQChWCGC_q5wiTUPFPmG1Fl30SS?usp=sharing'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-google" viewBox="0 0 16 16">
      <path d="M15.545 6.558a9.42 9.42 0 0 1 .139 1.626c0 2.434-.87 4.492-2.384 5.885h.002C11.978 15.292 10.158 16 8 16A8 8 0 1 1 8 0a7.689 7.689 0 0 1 5.352 2.082l-2.284 2.284A4.347 4.347 0 0 0 8 3.166c-2.087 0-3.86 1.408-4.492 3.304a4.792 4.792 0 0 0 0 3.063h.003c.635 1.893 2.405 3.301 4.492 3.301 1.078 0 2.004-.276 2.722-.764h-.003a3.702 3.702 0 0 0 1.599-2.431H8v-3.08h7.545z"></path>
      </svg>
        Collab
      </button>

    </p>
    <div id ="center_img">
      <img src="images/arch.jpeg" style="width:60%">
    </div>
    <p class="fig">
      We use graph neural networks with unsupervised losses from classical graph theory to solve various image segmentation tasks.
      We use deep features from a pre-trained vision transformer (ViT) as input to our network, thus avoiding expensive training associated with end-to-end methods.
    </p>

    
    <p class="sub_title">Abstract</p>
    
    <p class="text">
      Image segmentation is a fundamental task in computer vision.
      Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods.
      Current approaches often rely on extracting deep features from pre-trained networks to construct a graph, and classical clustering methods like k-means and normalized-cuts are then applied as a post-processing step. However, this approach reduces the high-dimensional information encoded in the features to pair-wise scalar affinities.
      To address this limitation, this study introduces a lightweight Graph Neural Network (GNN) to replace classical clustering methods while optimizing for the same clustering objective function. Unlike existing methods, our GNN takes both the pair-wise affinities between local image features and the raw features as input. This direct connection between the raw features and the clustering objective enables us to implicitly perform classification of the clusters between different graphs, resulting in part semantic segmentation without the need for additional post-processing steps.
      We demonstrate how classical clustering objectives can be formulated as self-supervised loss functions for training an image segmentation GNN. Furthermore, we employ the Correlation-Clustering (CC) objective to perform clustering without defining the number of clusters, allowing for k-less clustering.
      We apply the proposed method for object localization, segmentation, and semantic part segmentation tasks, surpassing state-of-the-art performance on multiple benchmarks.
    </p>

    <p class="sub_title">Examples</p>
    <p class="text">
    All examples were produced using DeepCut without post-processing steps such as CRF, CAD, bilateral solver, etc.
    </p>


    <p class="sub_sub_title">Single object segmentation</p>
    <p class="text">
    We apply clustering to the image data into two groups with different semantic meanings.
    We chose the largest connect component to be our segmented object. For object detection, we
    draw a bounding box around the selected object.</p>

    <div id ="center_img">
      <img src="images/bird1.png" style="width:30%">
      <img src="images/bird2.png" style="width:30%">
    </div>
    <p class="fig">
      The original image with bounding box created using DeepCut (left) and single object segmentation using DeepCut (right). 
    </p>

    <div id ="center_img">
      <img src="images/web_single2.jpeg" style="width:60%">
    </div>

    <div id ="center_img">
      <img src="images/web_single.jpeg" style="width:60%">
    </div>

    <p class="fig">
      Pairs of images (top) and its foreground-background segmentation (bottom). 
      In order to achieve single object segmentation for cases such as the bottom left image, we chose one cluster (the largest connected component in the graph).
    </p>

    <p class="sub_sub_title">Semantic part segmentation</p>
    <div id ="center_img">
      <img src="images/sem.jpeg" style="width:60%">
    </div>
    <p class="fig">
      Semantic part segmentation across all three images using DeepCut with 2-stage segmentation as described in the paper. 
      Top: original image, middle: foreground part segmentation, bottom: foreground and background part segmentation.
    </p>

    <div id ="center_img">
      <img src="images/web_sem.jpeg" style="width:60%">
    </div>
    <p class="fig">
      Examples of semantic part segmentation. Note that here the segmentation is preformed on each image separately.
    </p>

    <p class="sub_sub_title">Video semantic part segmentation</p>
    </p>
    <div id ="center_img">
      <img src="images/dog.gif" style="width:30%">
      <img src="images/dog_mask.gif" style="width:30%">
    </div>
    <p class="fig">
      Video of semantic part segmentation, note the implicit tracking of the part segmentation.
    </p>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
    <p class="ak">
      acknowledgments
    </p>
    <p class="fig">
      Special thanks to the SAMPL AI team: Dudi Radosezki, Ariel Keslassy, Lital Binyamin, Chen Solomon, Yishai Schlesinger, Michal Katirai and Hana Hasan.
    </p>

  </body>
</html>

