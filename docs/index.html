<!doctype html>
<html lang="en">
  <head>
    
    <link rel="stylesheet" href="style.css">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DeepCut</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi" crossorigin="anonymous">
    <style>
      a:link {
         color: rgb(0, 0, 0);
         background-color: transparent;
         text-decoration: none;
      }
  </style>
  </head>
  <body>
    <p class="title">DeepCut: Unsupervised Segmentation using Graph Neural Networks Clustering</p>
    <p class="author">
      <span class="author"><a target="_blank" href="https://www.linkedin.com/in/amit-aflalo-8792b7215/">Amit Aflalo</a></span>
      <span class="author"><a target="_blank" href="https://www.weizmann.ac.il/math/bagon/home">Shai Bagon</a></span>
      <span class="author"><a target="_blank" href="https://www.linkedin.com/in/tamar-kashti-35a8071">Tamar Kashti</a></span>
      <span class="author"><a target="_blank" href="https://www.weizmann.ac.il/math/yonina/about-me/yonina-eldar">Yonina Eldar</a></span>

    </p>
    <div id ="center_img">
      <a href="https://www.weizmann.ac.il/math/yonina/signal-acquisition-modeling-processing-and-learning-sampl-lab" target="_blank"><img src="images/SAMPLLAB.png" style="width:17.5%"></a>
      <a href="https://www.weizmann.ac.il/" target="_blank"><img src="images/wis2.jpg" style="width:30%"></a>
    </div>

    <p class="icons">
      <button disabled onclick="location.href='https://github.com/amitt1236/'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-github" viewBox="0 0 16 16">
      <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
      </svg>
        GitHub
      </button>
      <button onclick="location.href='https://arxiv.org/abs/2212.05853'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-newspaper" viewBox="0 0 16 16">
      <path d="M0 2.5A1.5 1.5 0 0 1 1.5 1h11A1.5 1.5 0 0 1 14 2.5v10.528c0 .3-.05.654-.238.972h.738a.5.5 0 0 0 .5-.5v-9a.5.5 0 0 1 1 0v9a1.5 1.5 0 0 1-1.5 1.5H1.497A1.497 1.497 0 0 1 0 13.5v-11zM12 14c.37 0 .654-.211.853-.441.092-.106.147-.279.147-.531V2.5a.5.5 0 0 0-.5-.5h-11a.5.5 0 0 0-.5.5v11c0 .278.223.5.497.5H12z"></path>
      <path d="M2 3h10v2H2V3zm0 3h4v3H2V6zm0 4h4v1H2v-1zm0 2h4v1H2v-1zm5-6h2v1H7V6zm3 0h2v1h-2V6zM7 8h2v1H7V8zm3 0h2v1h-2V8zm-3 2h2v1H7v-1zm3 0h2v1h-2v-1zm-3 2h2v1H7v-1zm3 0h2v1h-2v-1z"></path>
      </svg>
        Paper
      </button>
      <button disabled onclick="location.href='https://colab.research.google.com/'" type="button" class="btn btn-outline-danger">
        <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-google" viewBox="0 0 16 16">
      <path d="M15.545 6.558a9.42 9.42 0 0 1 .139 1.626c0 2.434-.87 4.492-2.384 5.885h.002C11.978 15.292 10.158 16 8 16A8 8 0 1 1 8 0a7.689 7.689 0 0 1 5.352 2.082l-2.284 2.284A4.347 4.347 0 0 0 8 3.166c-2.087 0-3.86 1.408-4.492 3.304a4.792 4.792 0 0 0 0 3.063h.003c.635 1.893 2.405 3.301 4.492 3.301 1.078 0 2.004-.276 2.722-.764h-.003a3.702 3.702 0 0 0 1.599-2.431H8v-3.08h7.545z"></path>
      </svg>
        Collab
      </button>

    </p>
    <div id ="center_img">
      <img src="images/arch.jpeg" style="width:60%">
    </div>
    <p class="fig">
      We use graph neural networks with unsupervised losses from classical graph theory to solve various image segmentation tasks.
      We use deep features from a pre-trained vision transformer (ViT) as input to our network, thus avoiding expensive training associated with end-to-end methods.
    </p>

    
    <p class="sub_title">Abstract</p>
    
    <p class="text">Image segmentation is a fundamental task in computer vision.
      Data annotation for training supervised methods can be labor-intensive, motivating unsupervised methods.
      Some existing approaches extract deep features from pre-trained networks and build a graph to apply classical clustering methods (e.g., k-means and normalized-cuts) as a post-processing stage. 
      These techniques reduce the high-dimensional information encoded in the features to pair-wise scalar affinities.
      In this work, we replace classical clustering algorithms with a lightweight Graph Neural Network (GNN) 
      trained to achieve the same clustering objective function.
      However, in contrast to existing approaches, we feed the GNN not only the pair-wise affinities between local image features but also the raw features themselves.
      Maintaining this connection between the raw feature and the clustering goal allows to perform part semantic segmentation implicitly, without requiring additional post-processing steps.
      We demonstrate how classical clustering objectives can be formulated as self-supervised loss functions for training our image segmentation GNN.
      Additionally, we use the Correlation-Clustering (CC) objective
      to perform clustering without defining the number of clusters (k-less clustering).
      We apply the proposed method for object localization, segmentation, and semantic part segmentation tasks, surpassing state-of-the-art performance on multiple benchmarks.
    </p>

    <p class="sub_title">Examples</p>
    <p class="text">
    All examples were produced using DeepCut without post-processing steps such as CRF, CAD, bilateral solver, etc.
    </p>


    <p class="sub_sub_title">Single object segmentation</p>
    <p class="text">
    We apply clustering to the image data into two groups with different semantic meanings.
    We chose the largest connect component to be our segmented object. For object detection, we
    draw a bounding box around the selected object.</p>

    <div id ="center_img">
      <img src="images/bird1.png" style="width:30%">
      <img src="images/bird2.png" style="width:30%">
    </div>
    <p class="fig">
      The original image with bounding box created using DeepCut (left) and single object segmentation using DeepCut (right). 
    </p>

    <div id ="center_img">
      <img src="images/web_single2.jpeg" style="width:60%">
    </div>

    <div id ="center_img">
      <img src="images/web_single.jpeg" style="width:60%">
    </div>

    <p class="fig">
      Pairs of images (top) and its foreground-background segmentation (bottom). 
      In order to achieve single object segmentation for cases such as the bottom left image, we chose one cluster (the largest connected component in the graph).
    </p>

    <p class="sub_sub_title">Semantic part segmentation</p>
    <div id ="center_img">
      <img src="images/sem.jpeg" style="width:60%">
    </div>
    <p class="fig">
      Semantic part segmentation across all three images using DeepCut with 2-stage segmentation as described in the paper. 
      Top: original image, middle: foreground part segmentation, bottom: foreground and background part segmentation.
    </p>

    <div id ="center_img">
      <img src="images/web_sem.jpeg" style="width:60%">
    </div>
    <p class="fig">
      Examples of semantic part segmentation. Note that here the segmentation is preformed on each image separately.
    </p>

    <p class="sub_sub_title">Video semantic part segmentation</p>
    </p>
    <div id ="center_img">
      <img src="images/dog.gif" style="width:30%">
      <img src="images/dog_mask.gif" style="width:30%">
    </div>
    <p class="fig">
      Video of semantic part segmentation, note the implicit tracking of the part segmentation.
    </p>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-OERcA2EqjJCMA+/3y+gxIOqMEjwtxJY7qPCqsdltbNJuaOe923+mo//f6V8Qbsw3" crossorigin="anonymous"></script>
    <p class="ak">
      acknowledgments
    </p>
    <p class="fig">
      Special thanks to the SAMPL AI team: Dudi Radosezki, Ariel Keslassy, Lital Binyamin, Chen Solomon, Yishai Schlesinger, Michal Katirai and Hana Hasan.
    </p>

  </body>
</html>

